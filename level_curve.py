import pandas as pd
from scipy.stats import pearsonr
import matplotlib.pyplot as plt

# # LEVEL CURVE 1 LINE
# ===== LOAD CSV =====
# df = pd.read_csv("/Users/hoangnguyen/Documents/py/ArrowPuzzle/levels_result_with_difficulty_1612.csv")

# # N·∫øu DifficultyLabel l√† text ‚Üí map sang s·ªë
# if df["DifficultyLabel"].dtype == object:
#     unique_labels = sorted(df["DifficultyLabel"].unique())
#     label_map = {label: i+1 for i, label in enumerate(unique_labels)}
#     df["DifficultyLabel_Num"] = df["DifficultyLabel"].map(label_map)
# else:
#     df["DifficultyLabel_Num"] = df["DifficultyLabel"]

# # ===== T√çNH MAE (Mean Absolute Error) =====
# mae = (df["DifficultyLabel"] - df["DifficultyLabel_Num"]).abs().mean()

# # ===== T√çNH PEARSON CORRELATION =====
# pearson_corr, p_value = pearsonr(df["DifficultyLabel"], df["DifficultyLabel_Num"])

# # ===== PRINT RESULT =====
# print("========== DIFFICULTY METRICS ==========")
# print(f"MAE (Mean Absolute Error): {mae:.4f}")
# print(f"Pearson correlation (r): {pearson_corr:.4f}")
# print(f"P-value: {p_value:.6f}")
# print("========================================")

# # LEVEL CURVE 2 LINE
# # ================================
# # LOAD CSV
# # ================================
df = pd.read_csv("D:\py\ArrowPuzzle\difficulty_plots\inference_output.csv")
df['Win Rate'] = pd.cut(df['Win Rate'], 
                        bins=[0, 20, 40, 60, 80, 100], 
                        labels=[5, 4, 3, 2, 1], 
                        include_lowest=True).astype(int)
# Chuy·ªÉn DifficultyLabel sang s·ªë n·∫øu l√† text
if df["predicted_stars"].dtype == object:
    unique_labels = sorted(df["predicted_stars"].unique())
    label_map = {label: i+1 for i, label in enumerate(unique_labels)}
    df["predicted_stars"] = df["predicted_stars"].map(label_map)
else:
    df["predicted_stars"] = df["predicted_stars"]

# ================================
# T√çNH MAE
# ================================
mae = (df["predicted_stars"] - df["Win Rate"]).abs().mean()

# ================================
# T√çNH PEARSON CORRELATION
# ================================
pearson_corr, p_value = pearsonr(df["predicted_stars"], df["Win Rate"])

print("\n========== DIFFICULTY METRICS ==========")
print(f"ƒê·ªô l·ªách trung b√¨nh tuy·ªát ƒë·ªëi: {mae:.4f}")
print(f"H·ªá s·ªë t∆∞∆°ng quan Pearson: {pearson_corr:.4f}")
print(f"P-value: {p_value:.6f}")
print("========================================")

# ================================
# V·∫º BI·ªÇU ƒê·ªí SO S√ÅNH 2 ƒê∆Ø·ªúNG
# ================================
plt.figure(figsize=(18, 5))

x = range(1, len(df) + 1)

plt.plot(
    x, df["predicted_stars"],
    marker='o', linestyle='-', color='darkblue',
    label="ƒê·ªô kh√≥ t·ª± gi·∫£i"
)

plt.plot(
    x, df["Win Rate"],
    marker='s', linestyle='--', color='crimson',  #crimson
    label="ƒê·ªô kh√≥ th·ª±c t·∫ø"
)

plt.fill_between(x, df["predicted_stars"], df["Win Rate"], color='skyblue', alpha=0.15)

plt.xticks(x, df["Level_Name"], rotation=90, fontsize=7)
plt.yticks([1, 2, 3, 4, 5])
plt.xlabel("Level Name")
plt.ylabel("Difficulty Score")
plt.title("Level Curve")
plt.grid(True, linestyle='--', alpha=0.6)
plt.ylim(0.5, 5.5)
plt.legend()
plt.tight_layout()

plt.show()


# import pandas as pd
# from scipy.stats import pearsonr
# import matplotlib.pyplot as plt

# # ================================
# # LOAD CSV
# # ================================
# # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n file c·ªßa b·∫°n ·ªü ƒë√¢y
# import pandas as pd
# from scipy.stats import pearsonr
# import matplotlib.pyplot as plt
# import numpy as np

# # ================================
# # LOAD V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU
# # ================================
# def load_and_prepare_data(filepath, num_levels=50):
#     """Load v√† chu·∫©n b·ªã d·ªØ li·ªáu"""
#     df = pd.read_csv(filepath)
#     df = df.head(num_levels)
    
#     # Chuy·ªÉn DifficultyLabel sang s·ªë
#     if df["DifficultyLabel"].dtype == object:
#         unique_labels = sorted(df["DifficultyLabel"].unique())
#         label_map = {label: i+1 for i, label in enumerate(unique_labels)}
#         df["DifficultyLabel_Num"] = df["DifficultyLabel"].map(label_map)
#     else:
#         df["DifficultyLabel_Num"] = df["DifficultyLabel"]
    
#     # X·ª≠ l√Ω Win Rate
#     if df["Win Rate"].dtype == object:
#         df["Win Rate"] = df["Win Rate"].str.replace('%', '').astype(float)
    
#     # Chuy·ªÉn Win Rate v·ªÅ thang 5
#     df["Win_Rate_Score"] = pd.cut(
#         df["Win Rate"],
#         bins=[0, 20, 40, 60, 80, 100],
#         labels=[5, 4, 3, 2, 1],
#         include_lowest=True
#     ).astype(int)
    
#     return df

# # ================================
# # H√ÄM T√çNH METRICS
# # ================================
# def calculate_metrics(data1, data2, name1, name2):
#     """
#     T√≠nh MAE v√† Pearson correlation gi·ªØa 2 series
    
#     Returns:
#     --------
#     dict: {'mae': float, 'pearson_r': float, 'p_value': float, ...}
#     """
#     # MAE (Mean Absolute Error)
#     mae = (data1 - data2).abs().mean()
    
#     # Pearson correlation
#     pearson_r, p_value = pearsonr(data1, data2)
    
#     # RMSE (Root Mean Square Error)
#     rmse = np.sqrt(((data1 - data2) ** 2).mean())
    
#     # Max deviation
#     max_dev = (data1 - data2).abs().max()
    
#     # Correlation strength
#     if abs(pearson_r) >= 0.8:
#         strength = "R·∫•t m·∫°nh"
#     elif abs(pearson_r) >= 0.6:
#         strength = "M·∫°nh"
#     elif abs(pearson_r) >= 0.4:
#         strength = "Trung b√¨nh"
#     elif abs(pearson_r) >= 0.2:
#         strength = "Y·∫øu"
#     else:
#         strength = "R·∫•t y·∫øu"
    
#     return {
#         'name1': name1,
#         'name2': name2,
#         'mae': mae,
#         'rmse': rmse,
#         'max_deviation': max_dev,
#         'pearson_r': pearson_r,
#         'p_value': p_value,
#         'correlation_strength': strength
#     }

# # ================================
# # H√ÄM PLOT 2 ƒê∆Ø·ªúNG V·ªöI METRICS
# # ================================
# def plot_two_lines_with_metrics(df, 
#                                  line1_name, line2_name,
#                                  show_metrics=True,
#                                  show_metrics_on_plot=True,
#                                  filename=None):
#     """
#     Plot 2 ƒë∆∞·ªùng v√† t√≠nh to√°n metrics
    
#     Parameters:
#     -----------
#     df : DataFrame
#         D·ªØ li·ªáu
#     line1_name : str
#         'difficulty', 'difficulty_label', ho·∫∑c 'winrate'
#     line2_name : str
#         'difficulty', 'difficulty_label', ho·∫∑c 'winrate'
#     show_metrics : bool
#         In metrics ra console
#     show_metrics_on_plot : bool
#         Hi·ªÉn th·ªã metrics tr√™n bi·ªÉu ƒë·ªì
#     filename : str
#         T√™n file output (n·∫øu None s·∫Ω t·ª± ƒë·ªông t·∫°o)
#     """
    
#     # Map line names to data
#     line_config = {
#         'difficulty': {
#             'data': df['difficulty'],
#             'name': 'ƒê·ªô kh√≥ t·ª± gi·∫£i',
#             'color': 'darkblue',
#             'marker': 'o'
#         },
#         'difficulty_label': {
#             'data': df['DifficultyLabel_Num'],
#             'name': 'ƒê·ªô kh√≥ c·∫•u tr√∫c',
#             'color': 'crimson',
#             'marker': 's'
#         },
#         'winrate': {
#             'data': df['Win_Rate_Score'],
#             'name': 'Win Rate Score',
#             'color': 'forestgreen',
#             'marker': '^'
#         }
#     }
    
#     if line1_name not in line_config or line2_name not in line_config:
#         print("‚ùå T√™n ƒë∆∞·ªùng kh√¥ng h·ª£p l·ªá. Ch·ªçn: 'difficulty', 'difficulty_label', 'winrate'")
#         return None
    
#     line1 = line_config[line1_name]
#     line2 = line_config[line2_name]
    
#     # T√≠nh metrics
#     metrics = calculate_metrics(
#         line1['data'], line2['data'],
#         line1['name'], line2['name']
#     )
    
#     # Print metrics
#     if show_metrics:
#         print("\n" + "="*70)
#         print(f"METRICS: {metrics['name1']} vs {metrics['name2']}")
#         print("="*70)
#         print(f"MAE (Mean Absolute Error):     {metrics['mae']:.4f}")
#         print(f"RMSE (Root Mean Square Error): {metrics['rmse']:.4f}")
#         print(f"Max Deviation:                 {metrics['max_deviation']:.4f}")
#         print(f"Pearson r:                     {metrics['pearson_r']:.4f}")
#         print(f"P-value:                       {metrics['p_value']:.6f}")
#         print(f"Correlation Strength:          {metrics['correlation_strength']}")
#         print("="*70)
    
#     # Create plot
#     fig, ax = plt.subplots(figsize=(20, 7))
#     x = range(1, len(df) + 1)
    
#     # Plot lines
#     ax.plot(x, line1['data'], 
#             marker=line1['marker'], linestyle='-', color=line1['color'],
#             linewidth=2.5, label=line1['name'], markersize=5)
    
#     ax.plot(x, line2['data'],
#             marker=line2['marker'], linestyle='--', color=line2['color'],
#             linewidth=2.5, label=line2['name'], markersize=5)
    
#     # Fill area
#     ax.fill_between(x, line1['data'], line2['data'], 
#                      alpha=0.2, color=line2['color'])
    
#     # Setup plot
#     ax.set_xticks(x)
#     ax.set_xticklabels(df["Level_Name"], rotation=90, fontsize=8, ha='right')
#     ax.set_yticks([1, 2, 3, 4, 5])
#     ax.set_xlabel("Level Name", fontsize=12, fontweight='bold')
#     ax.set_ylabel("Score (1-5)", fontsize=12, fontweight='bold')
#     ax.set_ylim(0.5, 5.5)
#     ax.grid(True, linestyle='--', alpha=0.4)
    
#     # Title with metrics
#     if show_metrics_on_plot:
#         title = f"{line1['name']} vs {line2['name']}\n"
#         title += f"MAE: {metrics['mae']:.4f} | RMSE: {metrics['rmse']:.4f} | "
#         title += f"Pearson r: {metrics['pearson_r']:.4f} (p={metrics['p_value']:.6f})"
#         ax.set_title(title, fontsize=12, fontweight='bold', pad=15)
#     else:
#         ax.set_title(f"{line1['name']} vs {line2['name']}", 
#                      fontsize=14, fontweight='bold')
    
#     # Legend
#     ax.legend(fontsize=11, loc='upper left')
    
#     plt.tight_layout()
    
#     # Save
#     if filename is None:
#         filename = f"plot_{line1_name}_vs_{line2_name}_with_metrics.png"
    
#     filepath = f'D:\py\ArrowPuzzle\difficulty_plots/{filename}'
#     plt.savefig(filepath, dpi=300, bbox_inches='tight')
#     print(f"‚úì ƒê√£ l∆∞u: {filename}")
#     plt.show()
    
#     return metrics

# # ================================
# # H√ÄM PH√ÇN T√çCH T·∫§T C·∫¢ C√ÅC C·∫∂P
# # ================================
# def analyze_all_pairs(df, save_summary=True):
#     """
#     Ph√¢n t√≠ch v√† plot t·∫•t c·∫£ c√°c c·∫∑p 2 ƒë∆∞·ªùng
#     """
#     pairs = [
#         ('difficulty', 'difficulty_label'),
#         ('difficulty', 'winrate'),
#         ('difficulty_label', 'winrate')
#     ]
    
#     all_metrics = []
    
#     print("\n" + "="*80)
#     print("PH√ÇN T√çCH T·∫§T C·∫¢ C√ÅC C·∫∂P 2 ƒê∆Ø·ªúNG")
#     print("="*80)
    
#     for i, (line1, line2) in enumerate(pairs, 1):
#         print(f"\n[{i}/{len(pairs)}] Analyzing {line1} vs {line2}...")
#         metrics = plot_two_lines_with_metrics(
#             df, line1, line2,
#             show_metrics=True,
#             show_metrics_on_plot=True,
#             filename=f"metrics_{line1}_vs_{line2}.png"
#         )
#         all_metrics.append(metrics)
    
#     # T·∫°o b·∫£ng t·ªïng h·ª£p
#     print("\n" + "="*80)
#     print("B·∫¢NG T·ªîNG H·ª¢P METRICS")
#     print("="*80)
    
#     metrics_df = pd.DataFrame(all_metrics)
#     print("\n" + metrics_df.to_string(index=False))
    
#     # Ph√¢n t√≠ch so s√°nh
#     print("\n" + "="*80)
#     print("PH√ÇN T√çCH SO S√ÅNH")
#     print("="*80)
    
#     print("\n1. ƒê·ªò L·ªÜCH (MAE):")
#     print("-" * 80)
#     sorted_by_mae = sorted(all_metrics, key=lambda x: x['mae'])
#     for m in sorted_by_mae:
#         print(f"   {m['name1']:25s} vs {m['name2']:25s}: MAE = {m['mae']:.4f}")
    
#     print(f"\n   ‚Üí C·∫∑p G·∫¶N NH·∫§T (MAE th·∫•p nh·∫•t): {sorted_by_mae[0]['name1']} vs {sorted_by_mae[0]['name2']}")
#     print(f"   ‚Üí C·∫∑p XA NH·∫§T (MAE cao nh·∫•t): {sorted_by_mae[-1]['name1']} vs {sorted_by_mae[-1]['name2']}")
    
#     print("\n2. T∆Ø∆†NG QUAN (Pearson r):")
#     print("-" * 80)
#     sorted_by_corr = sorted(all_metrics, key=lambda x: abs(x['pearson_r']), reverse=True)
#     for m in sorted_by_corr:
#         sig = "***" if m['p_value'] < 0.001 else "**" if m['p_value'] < 0.01 else "*" if m['p_value'] < 0.05 else "ns"
#         print(f"   {m['name1']:25s} vs {m['name2']:25s}: r = {m['pearson_r']:7.4f} ({m['correlation_strength']:12s}) {sig}")
    
#     print(f"\n   ‚Üí T∆∞∆°ng quan M·∫†NH NH·∫§T: {sorted_by_corr[0]['name1']} vs {sorted_by_corr[0]['name2']}")
#     print(f"   ‚Üí T∆∞∆°ng quan Y·∫æU NH·∫§T: {sorted_by_corr[-1]['name1']} vs {sorted_by_corr[-1]['name2']}")
    
#     # T·∫°o bi·ªÉu ƒë·ªì so s√°nh
#     create_comparison_chart(all_metrics)
    
#     # Save summary
#     if save_summary:
#         metrics_df.to_csv('D:\py\ArrowPuzzle\difficulty_plots/metrics_summary_all_pairs.csv', index=False)
#         print("\n‚úì ƒê√£ l∆∞u: metrics_summary_all_pairs.csv")
    
#     return all_metrics

# # ================================
# # H√ÄM T·∫†O BI·ªÇU ƒê·ªí SO S√ÅNH
# # ================================
# def create_comparison_chart(all_metrics):
#     """T·∫°o bi·ªÉu ƒë·ªì so s√°nh c√°c metrics"""
    
#     fig = plt.figure(figsize=(20, 12))
    
#     # T·∫°o grid 2x2
#     gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)
    
#     pairs_labels = [f"{m['name1']}\nvs\n{m['name2']}" for m in all_metrics]
#     x_pos = range(len(all_metrics))
    
#     # 1. MAE comparison
#     ax1 = fig.add_subplot(gs[0, 0])
#     maes = [m['mae'] for m in all_metrics]
#     colors1 = ['skyblue', 'lightgreen', 'lightcoral']
#     bars1 = ax1.bar(x_pos, maes, color=colors1, edgecolor='black', linewidth=1.5)
#     ax1.set_xticks(x_pos)
#     ax1.set_xticklabels(pairs_labels, fontsize=9)
#     ax1.set_ylabel("MAE", fontsize=11, fontweight='bold')
#     ax1.set_title("Mean Absolute Error (MAE)", fontsize=12, fontweight='bold')
#     ax1.grid(axis='y', linestyle='--', alpha=0.4)
#     for bar, mae in zip(bars1, maes):
#         ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
#                 f'{mae:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
    
#     # 2. RMSE comparison
#     ax2 = fig.add_subplot(gs[0, 1])
#     rmses = [m['rmse'] for m in all_metrics]
#     bars2 = ax2.bar(x_pos, rmses, color=colors1, edgecolor='black', linewidth=1.5)
#     ax2.set_xticks(x_pos)
#     ax2.set_xticklabels(pairs_labels, fontsize=9)
#     ax2.set_ylabel("RMSE", fontsize=11, fontweight='bold')
#     ax2.set_title("Root Mean Square Error (RMSE)", fontsize=12, fontweight='bold')
#     ax2.grid(axis='y', linestyle='--', alpha=0.4)
#     for bar, rmse in zip(bars2, rmses):
#         ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
#                 f'{rmse:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
    
#     # 3. Pearson r comparison
#     ax3 = fig.add_subplot(gs[1, 0])
#     pearson_rs = [m['pearson_r'] for m in all_metrics]
#     colors3 = ['skyblue' if r >= 0 else 'salmon' for r in pearson_rs]
#     bars3 = ax3.bar(x_pos, pearson_rs, color=colors3, edgecolor='black', linewidth=1.5)
#     ax3.set_xticks(x_pos)
#     ax3.set_xticklabels(pairs_labels, fontsize=9)
#     ax3.set_ylabel("Pearson r", fontsize=11, fontweight='bold')
#     ax3.set_title("H·ªá s·ªë t∆∞∆°ng quan Pearson", fontsize=12, fontweight='bold')
#     ax3.axhline(y=0, color='black', linestyle='-', linewidth=1)
#     ax3.grid(axis='y', linestyle='--', alpha=0.4)
#     ax3.set_ylim(-1, 1)
#     for bar, r in zip(bars3, pearson_rs):
#         height = bar.get_height()
#         va = 'bottom' if height >= 0 else 'top'
#         ax3.text(bar.get_x() + bar.get_width()/2., height,
#                 f'{r:.4f}', ha='center', va=va, fontweight='bold', fontsize=10)
    
#     # 4. Max Deviation comparison
#     ax4 = fig.add_subplot(gs[1, 1])
#     max_devs = [m['max_deviation'] for m in all_metrics]
#     bars4 = ax4.bar(x_pos, max_devs, color=colors1, edgecolor='black', linewidth=1.5)
#     ax4.set_xticks(x_pos)
#     ax4.set_xticklabels(pairs_labels, fontsize=9)
#     ax4.set_ylabel("Max Deviation", fontsize=11, fontweight='bold')
#     ax4.set_title("ƒê·ªô l·ªách t·ªëi ƒëa", fontsize=12, fontweight='bold')
#     ax4.grid(axis='y', linestyle='--', alpha=0.4)
#     for bar, dev in zip(bars4, max_devs):
#         ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
#                 f'{dev:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
    
#     plt.suptitle("So s√°nh Metrics cho t·∫•t c·∫£ c√°c c·∫∑p 2 ƒë∆∞·ªùng", 
#                  fontsize=14, fontweight='bold', y=0.995)
    
#     plt.savefig('D:\py\ArrowPuzzle\difficulty_plots/metrics_comparison_all.png', dpi=300, bbox_inches='tight')
#     print("\n‚úì ƒê√£ l∆∞u: metrics_comparison_all.png")
#     plt.show()

# # ================================
# # MAIN EXECUTION
# # ================================
# if __name__ == "__main__":
#     # Load data
#     filepath ="D:\\py\\ArrowPuzzle\\[ArrowPuzzle] ƒê√°nh gi√° ƒë·ªô kh√≥ - Sheet2.csv"
#     df = load_and_prepare_data(filepath, num_levels=50)
    
#     print("\n‚úì ƒê√£ load {} levels".format(len(df)))
    
#     # Ph√¢n t√≠ch t·∫•t c·∫£ c√°c c·∫∑p
#     all_metrics = analyze_all_pairs(df, save_summary=True)
    
#     print("\n" + "="*80)
#     print("‚úÖ HO√ÄN T·∫§T!")
#     print("="*80)
#     print("\nƒê√£ t·∫°o c√°c file:")
#     print("  1. metrics_difficulty_vs_difficulty_label.png")
#     print("  2. metrics_difficulty_vs_winrate.png")
#     print("  3. metrics_difficulty_label_vs_winrate.png")
#     print("  4. metrics_comparison_all.png")
#     print("  5. metrics_summary_all_pairs.csv")
#     print("="*80)

# import pandas as pd
# from scipy.stats import pearsonr
# import matplotlib.pyplot as plt
# import numpy as np

# # ================================
# # LOAD V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU
# # ================================
# def load_and_prepare_data(filepath, num_levels=50):
#     """Load v√† chu·∫©n b·ªã d·ªØ li·ªáu"""
#     df = pd.read_csv(filepath)
#     df = df.head(num_levels)
    
#     # Chuy·ªÉn DifficultyLabel sang s·ªë
#     if df["DifficultyLabel"].dtype == object:
#         unique_labels = sorted(df["DifficultyLabel"].unique())
#         label_map = {label: i+1 for i, label in enumerate(unique_labels)}
#         df["DifficultyLabel_Num"] = df["DifficultyLabel"].map(label_map)
#     else:
#         df["DifficultyLabel_Num"] = df["DifficultyLabel"]
    
#     # X·ª≠ l√Ω Win Rate
#     if df["Win Rate"].dtype == object:
#         df["Win Rate"] = df["Win Rate"].str.replace('%', '').astype(float)
    
#     # Chuy·ªÉn Win Rate v·ªÅ thang 5
#     df["Win_Rate_Score"] = pd.cut(
#         df["Win Rate"],
#         bins=[0, 20, 40, 60, 80, 100],
#         labels=[5, 4, 3, 2, 1],
#         include_lowest=True
#     ).astype(int)
    
#     return df

# # ================================
# # H√ÄM √ÅP D·ª§NG PATTERN (QUAN TR·ªåNG)
# # ================================
# def apply_difficulty_pattern(df, column_name='difficulty'):
#     """
#     √Åp d·ª•ng pattern: sau 1 level c√≥ ƒë·ªô kh√≥ > 1, th√¨ 3 level ti·∫øp theo c√≥ ƒë·ªô kh√≥ = 1
    
#     Logic:
#     - Qu√©t t·ª´ng level
#     - Khi g·∫∑p level c√≥ gi√° tr·ªã > 1:
#         + Gi·ªØ nguy√™n gi√° tr·ªã ƒë√≥
#         + 3 level TI·∫æP THEO s·∫Ω b·ªã set = 1
#     - Sau ƒë√≥ ti·∫øp t·ª•c qu√©t v√† l·∫∑p l·∫°i
    
#     Parameters:
#     -----------
#     df : DataFrame
#         D·ªØ li·ªáu g·ªëc
#     column_name : str
#         T√™n c·ªôt c·∫ßn √°p d·ª•ng pattern ('difficulty' ho·∫∑c 'DifficultyLabel_Num')
    
#     Returns:
#     --------
#     DataFrame v·ªõi c·ªôt m·ªõi ƒë√£ √°p d·ª•ng pattern
#     """
#     df_modified = df.copy()
#     new_column_name = f"{column_name}_pattern"
    
#     # Kh·ªüi t·∫°o c·ªôt m·ªõi v·ªõi gi√° tr·ªã g·ªëc
#     df_modified[new_column_name] = df_modified[column_name].copy()
    
#     # √Åp d·ª•ng pattern
#     i = 0
#     while i < len(df_modified):
#         current_value = df_modified.iloc[i][column_name]
        
#         # N·∫øu gi√° tr·ªã hi·ªán t·∫°i > 1
#         if current_value > 1:
#             # Gi·ªØ nguy√™n gi√° tr·ªã n√†y (ƒë√£ copy t·ª´ g·ªëc)
#             # Set 3 level TI·∫æP THEO = 1
#             for j in range(1, 3):  # 1, 2, 3
#                 if i + j < len(df_modified):
#                     df_modified.iloc[i + j, df_modified.columns.get_loc(new_column_name)] = 1
            
#             # Nh·∫£y sang level sau 3 level ƒë√£ set
#             i += 4  # Nh·∫£y qua level hi·ªán t·∫°i + 3 level ti·∫øp theo
#         else:
#             # N·∫øu <= 1, gi·ªØ nguy√™n v√† ti·∫øp t·ª•c
#             i += 1
    
#     return df_modified

# # ================================
# # H√ÄM T√çNH METRICS
# # ================================
# def calculate_metrics(data1, data2, name1, name2):
#     """
#     T√≠nh MAE v√† Pearson correlation gi·ªØa 2 series
    
#     Returns:
#     --------
#     dict: {'mae': float, 'pearson_r': float, 'p_value': float, ...}
#     """
#     # MAE (Mean Absolute Error)
#     mae = (data1 - data2).abs().mean()
    
#     # Pearson correlation
#     pearson_r, p_value = pearsonr(data1, data2)
    
#     # RMSE (Root Mean Square Error)
#     rmse = np.sqrt(((data1 - data2) ** 2).mean())
    
#     # Max deviation
#     max_dev = (data1 - data2).abs().max()
    
#     # Correlation strength
#     if abs(pearson_r) >= 0.8:
#         strength = "R·∫•t m·∫°nh"
#     elif abs(pearson_r) >= 0.6:
#         strength = "M·∫°nh"
#     elif abs(pearson_r) >= 0.4:
#         strength = "Trung b√¨nh"
#     elif abs(pearson_r) >= 0.2:
#         strength = "Y·∫øu"
#     else:
#         strength = "R·∫•t y·∫øu"
    
#     return {
#         'name1': name1,
#         'name2': name2,
#         'mae': mae,
#         'rmse': rmse,
#         'max_deviation': max_dev,
#         'pearson_r': pearson_r,
#         'p_value': p_value,
#         'correlation_strength': strength
#     }

# # ================================
# # H√ÄM PLOT PATTERN V·ªöI WIN RATE
# # ================================
# def plot_pattern_vs_winrate(df, difficulty_column='difficulty', show_original=True):
#     """
#     Plot so s√°nh: Difficulty Pattern vs Win Rate
#     C√≥ th·ªÉ hi·ªÉn th·ªã th√™m ƒë∆∞·ªùng g·ªëc ƒë·ªÉ tham kh·∫£o
    
#     Parameters:
#     -----------
#     df : DataFrame
#         D·ªØ li·ªáu ƒë√£ c√≥ c·ªôt pattern
#     difficulty_column : str
#         'difficulty' ho·∫∑c 'DifficultyLabel_Num'
#     show_original : bool
#         C√≥ hi·ªÉn th·ªã ƒë∆∞·ªùng g·ªëc kh√¥ng (m·ªù ƒëi ƒë·ªÉ tham kh·∫£o)
#     """
#     pattern_column = f"{difficulty_column}_pattern"
    
#     # T√≠nh metrics gi·ªØa pattern v√† win rate
#     metrics = calculate_metrics(
#         df[pattern_column],
#         df['Win_Rate_Score'],
#         f'{difficulty_column} (Pattern)',
#         'Win Rate Score'
#     )
    
#     # Print metrics
#     print("\n" + "="*70)
#     print(f"METRICS: {metrics['name1']} vs {metrics['name2']}")
#     print("="*70)
#     print(f"MAE (Mean Absolute Error):     {metrics['mae']:.4f}")
#     print(f"RMSE (Root Mean Square Error): {metrics['rmse']:.4f}")
#     print(f"Max Deviation:                 {metrics['max_deviation']:.4f}")
#     print(f"Pearson r:                     {metrics['pearson_r']:.4f}")
#     print(f"P-value:                       {metrics['p_value']:.6f}")
#     print(f"Correlation Strength:          {metrics['correlation_strength']}")
#     print("="*70)
    
#     # Create plot
#     fig, ax = plt.subplots(figsize=(20, 7))
#     x = range(1, len(df) + 1)
    
#     # Plot ƒë∆∞·ªùng g·ªëc (m·ªù ƒëi, ch·ªâ ƒë·ªÉ tham kh·∫£o)
#     if show_original:
#         ax.plot(x, df[difficulty_column], 
#                 marker='o', linestyle=':', color='gray',
#                 linewidth=1.5, label=f'{difficulty_column} (G·ªëc - tham kh·∫£o)', 
#                 markersize=4, alpha=0.4)
    
#     # Plot ƒë∆∞·ªùng pattern (ch√≠nh)
#     ax.plot(x, df[pattern_column],
#             marker='s', linestyle='-', color='crimson',
#             linewidth=2.5, label=f'{difficulty_column} (Pattern: 1 kh√≥ ‚Üí 3 d·ªÖ)', 
#             markersize=6)
    
#     # Plot Win Rate (th·ª±c t·∫ø)
#     ax.plot(x, df['Win_Rate_Score'],
#             marker='^', linestyle='--', color='forestgreen',
#             linewidth=2.5, label='Win Rate Score (Th·ª±c t·∫ø)', markersize=6)
    
#     # Fill area gi·ªØa pattern v√† win rate
#     ax.fill_between(x, df[pattern_column], df['Win_Rate_Score'], 
#                      alpha=0.15, color='purple')
    
#     # ƒê√°nh d·∫•u c√°c ƒëi·ªÉm b·ªã thay ƒë·ªïi b·ªüi pattern
#     for i in range(len(df)):
#         if df[difficulty_column].iloc[i] != df[pattern_column].iloc[i]:
#             ax.axvline(x=i+1, color='orange', alpha=0.3, linestyle=':', linewidth=1.5)
    
#     # Setup plot
#     ax.set_xticks(x)
#     ax.set_xticklabels(df["Level_Name"], rotation=90, fontsize=8, ha='right')
#     ax.set_yticks([1, 2, 3, 4, 5])
#     ax.set_xlabel("Level Name", fontsize=12, fontweight='bold')
#     ax.set_ylabel("Score (1-5)", fontsize=12, fontweight='bold')
#     ax.set_ylim(0.5, 5.5)
#     ax.grid(True, linestyle='--', alpha=0.4)
    
#     # Title with metrics
#     title = f"So s√°nh: {difficulty_column} (Pattern) vs Win Rate\n"
#     title += f"MAE: {metrics['mae']:.4f} | RMSE: {metrics['rmse']:.4f} | "
#     title += f"Pearson r: {metrics['pearson_r']:.4f} (p={metrics['p_value']:.6f})"
#     ax.set_title(title, fontsize=12, fontweight='bold', pad=15)
    
#     # Legend
#     ax.legend(fontsize=11, loc='upper left', framealpha=0.9)
    
#     plt.tight_layout()
    
#     # Save
#     filename = f"FINAL_pattern_{difficulty_column}_vs_winrate.png"
#     filepath = f'D:\\py\\ArrowPuzzle\\difficulty_plots\\{filename}'
#     plt.savefig(filepath, dpi=300, bbox_inches='tight')
#     print(f"‚úì ƒê√£ l∆∞u: {filename}")
#     plt.show()
    
#     return metrics

# # ================================
# # H√ÄM PLOT SO S√ÅNH G·ªêC VS PATTERN
# # ================================
# def plot_original_vs_pattern(df, column_name='difficulty'):
#     """
#     So s√°nh ƒë∆∞·ªùng g·ªëc v·ªõi ƒë∆∞·ªùng ƒë√£ √°p d·ª•ng pattern
#     """
#     pattern_column = f"{column_name}_pattern"
    
#     # T√≠nh metrics
#     metrics = calculate_metrics(
#         df[column_name], 
#         df[pattern_column],
#         f"{column_name} (G·ªëc)",
#         f"{column_name} (Pattern)"
#     )
    
#     # Print metrics
#     print("\n" + "="*70)
#     print(f"METRICS: {metrics['name1']} vs {metrics['name2']}")
#     print("="*70)
#     print(f"MAE (Mean Absolute Error):     {metrics['mae']:.4f}")
#     print(f"RMSE (Root Mean Square Error): {metrics['rmse']:.4f}")
#     print(f"Max Deviation:                 {metrics['max_deviation']:.4f}")
#     print(f"Pearson r:                     {metrics['pearson_r']:.4f}")
#     print(f"P-value:                       {metrics['p_value']:.6f}")
#     print(f"Correlation Strength:          {metrics['correlation_strength']}")
#     print("="*70)
    
#     # Create plot
#     fig, ax = plt.subplots(figsize=(20, 7))
#     x = range(1, len(df) + 1)
    
#     # Plot ƒë∆∞·ªùng g·ªëc
#     ax.plot(x, df[column_name], 
#             marker='o', linestyle='-', color='darkblue',
#             linewidth=2.5, label=f'{column_name} (G·ªëc)', markersize=5)
    
#     # Plot ƒë∆∞·ªùng pattern
#     ax.plot(x, df[pattern_column],
#             marker='s', linestyle='--', color='crimson',
#             linewidth=2.5, label=f'{column_name} (Pattern: 1 kh√≥ ‚Üí 3 d·ªÖ)', markersize=5)
    
#     # Fill area gi·ªØa 2 ƒë∆∞·ªùng
#     ax.fill_between(x, df[column_name], df[pattern_column], 
#                      alpha=0.2, color='orange')
    
#     # ƒê√°nh d·∫•u c√°c ƒëi·ªÉm thay ƒë·ªïi
#     for i in range(len(df)):
#         if df[column_name].iloc[i] != df[pattern_column].iloc[i]:
#             ax.axvline(x=i+1, color='red', alpha=0.2, linestyle=':', linewidth=1)
#             # Th√™m annotation cho ƒëi·ªÉm thay ƒë·ªïi
#             ax.text(i+1, df[column_name].iloc[i], f'{df[column_name].iloc[i]}‚Üí1', 
#                    fontsize=7, ha='center', va='bottom', color='red', fontweight='bold')
    
#     # Setup plot
#     ax.set_xticks(x)
#     ax.set_xticklabels(df["Level_Name"], rotation=90, fontsize=8, ha='right')
#     ax.set_yticks([1, 2, 3, 4, 5])
#     ax.set_xlabel("Level Name", fontsize=12, fontweight='bold')
#     ax.set_ylabel("Score (1-5)", fontsize=12, fontweight='bold')
#     ax.set_ylim(0.5, 5.5)
#     ax.grid(True, linestyle='--', alpha=0.4)
    
#     # Title with metrics
#     title = f"So s√°nh {column_name}: G·ªëc vs Pattern (1 kh√≥ ‚Üí 3 d·ªÖ)\n"
#     title += f"MAE: {metrics['mae']:.4f} | RMSE: {metrics['rmse']:.4f} | "
#     title += f"Pearson r: {metrics['pearson_r']:.4f}"
#     ax.set_title(title, fontsize=12, fontweight='bold', pad=15)
    
#     # Legend
#     ax.legend(fontsize=11, loc='upper left')
    
#     plt.tight_layout()
    
#     # Save
#     filename = f"comparison_{column_name}_original_vs_pattern.png"
#     filepath = f'D:\\py\\ArrowPuzzle\\difficulty_plots\\{filename}'
#     plt.savefig(filepath, dpi=300, bbox_inches='tight')
#     print(f"‚úì ƒê√£ l∆∞u: {filename}")
#     plt.show()
    
#     return metrics

# # ================================
# # H√ÄM PH√ÇN T√çCH TO√ÄN DI·ªÜN
# # ================================
# def analyze_pattern_comprehensive(df):
#     """
#     Ph√¢n t√≠ch to√†n di·ªán pattern cho c·∫£ difficulty v√† difficulty_label
#     """
#     print("\n" + "="*80)
#     print("PH√ÇN T√çCH PATTERN: 1 KH√ì ‚Üí 3 D·ªÑ")
#     print("="*80)
    
#     # √Åp d·ª•ng pattern cho c·∫£ 2 c·ªôt
#     print("\nüìä ƒêang √°p d·ª•ng pattern...")
#     df_with_pattern = apply_difficulty_pattern(df, 'difficulty')
#     df_with_pattern = apply_difficulty_pattern(df_with_pattern, 'DifficultyLabel_Num')
    
#     # Hi·ªÉn th·ªã b·∫£ng so s√°nh
#     print("\n" + "="*80)
#     print("B·∫¢NG D·ªÆ LI·ªÜU SO S√ÅNH (10 levels ƒë·∫ßu)")
#     print("="*80)
#     display_df = df_with_pattern[['Level_Name', 'difficulty', 'difficulty_pattern', 
#                                     'DifficultyLabel_Num', 'DifficultyLabel_Num_pattern', 
#                                     'Win_Rate_Score']].head(10)
#     print(display_df.to_string(index=False))
    
#     # 1. So s√°nh difficulty g·ªëc vs pattern
#     print("\n" + "="*80)
#     print("[1] DIFFICULTY: G·ªëc vs Pattern")
#     print("="*80)
#     metrics_diff = plot_original_vs_pattern(df_with_pattern, 'difficulty')
    
#     # 2. So s√°nh difficulty_label g·ªëc vs pattern
#     print("\n" + "="*80)
#     print("[2] DIFFICULTY LABEL: G·ªëc vs Pattern")
#     print("="*80)
#     metrics_label = plot_original_vs_pattern(df_with_pattern, 'DifficultyLabel_Num')
    
#     # 3. Plot difficulty pattern v·ªõi Win Rate
#     print("\n" + "="*80)
#     print("[3] DIFFICULTY PATTERN vs WIN RATE")
#     print("="*80)
#     metrics_diff_wr = plot_pattern_vs_winrate(df_with_pattern, 'difficulty', show_original=True)
    
#     # 4. Plot difficulty_label pattern v·ªõi Win Rate
#     print("\n" + "="*80)
#     print("[4] DIFFICULTY LABEL PATTERN vs WIN RATE")
#     print("="*80)
#     metrics_label_wr = plot_pattern_vs_winrate(df_with_pattern, 'DifficultyLabel_Num', show_original=True)
    
#     # 5. T·ªïng h·ª£p so s√°nh
#     print("\n" + "="*80)
#     print("T·ªîNG H·ª¢P K·∫æT QU·∫¢")
#     print("="*80)
    
#     # So s√°nh v·ªõi d·ªØ li·ªáu g·ªëc
#     original_diff_wr_mae = (df_with_pattern['difficulty'] - df_with_pattern['Win_Rate_Score']).abs().mean()
#     original_label_wr_mae = (df_with_pattern['DifficultyLabel_Num'] - df_with_pattern['Win_Rate_Score']).abs().mean()
    
#     print("\nüìà So s√°nh MAE v·ªõi Win Rate:")
#     print("-" * 80)
#     print(f"  Difficulty (G·ªëc):           {original_diff_wr_mae:.4f}")
#     print(f"  Difficulty (Pattern):       {metrics_diff_wr['mae']:.4f}")
#     change_diff = metrics_diff_wr['mae'] - original_diff_wr_mae
#     pct_diff = (change_diff / original_diff_wr_mae * 100)
#     print(f"  ‚Üí Thay ƒë·ªïi:                 {change_diff:+.4f} ({pct_diff:+.2f}%)")
#     if change_diff < 0:
#         print(f"  ‚úÖ Pattern C·∫¢I THI·ªÜN ƒë·ªô ch√≠nh x√°c!")
#     else:
#         print(f"  ‚ùå Pattern l√†m GI·∫¢M ƒë·ªô ch√≠nh x√°c!")
    
#     print(f"\n  Difficulty Label (G·ªëc):     {original_label_wr_mae:.4f}")
#     print(f"  Difficulty Label (Pattern): {metrics_label_wr['mae']:.4f}")
#     change_label = metrics_label_wr['mae'] - original_label_wr_mae
#     pct_label = (change_label / original_label_wr_mae * 100)
#     print(f"  ‚Üí Thay ƒë·ªïi:                 {change_label:+.4f} ({pct_label:+.2f}%)")
#     if change_label < 0:
#         print(f"  ‚úÖ Pattern C·∫¢I THI·ªÜN ƒë·ªô ch√≠nh x√°c!")
#     else:
#         print(f"  ‚ùå Pattern l√†m GI·∫¢M ƒë·ªô ch√≠nh x√°c!")
    
#     # So s√°nh t∆∞∆°ng quan
#     print("\nüìä So s√°nh T∆∞∆°ng quan Pearson v·ªõi Win Rate:")
#     print("-" * 80)
#     original_diff_wr_corr = pearsonr(df_with_pattern['difficulty'], df_with_pattern['Win_Rate_Score'])[0]
#     original_label_wr_corr = pearsonr(df_with_pattern['DifficultyLabel_Num'], df_with_pattern['Win_Rate_Score'])[0]
    
#     print(f"  Difficulty (G·ªëc):           {original_diff_wr_corr:.4f}")
#     print(f"  Difficulty (Pattern):       {metrics_diff_wr['pearson_r']:.4f}")
#     print(f"  ‚Üí Thay ƒë·ªïi:                 {metrics_diff_wr['pearson_r'] - original_diff_wr_corr:+.4f}")
    
#     print(f"\n  Difficulty Label (G·ªëc):     {original_label_wr_corr:.4f}")
#     print(f"  Difficulty Label (Pattern): {metrics_label_wr['pearson_r']:.4f}")
#     print(f"  ‚Üí Thay ƒë·ªïi:                 {metrics_label_wr['pearson_r'] - original_label_wr_corr:+.4f}")
    
#     # S·ªë l∆∞·ª£ng level b·ªã thay ƒë·ªïi
#     print("\nüìù Th·ªëng k√™ thay ƒë·ªïi:")
#     print("-" * 80)
#     diff_changed = (df_with_pattern['difficulty'] != df_with_pattern['difficulty_pattern']).sum()
#     label_changed = (df_with_pattern['DifficultyLabel_Num'] != df_with_pattern['DifficultyLabel_Num_pattern']).sum()
    
#     print(f"  Difficulty: {diff_changed}/{len(df_with_pattern)} levels b·ªã thay ƒë·ªïi ({diff_changed/len(df_with_pattern)*100:.1f}%)")
#     print(f"  Difficulty Label: {label_changed}/{len(df_with_pattern)} levels b·ªã thay ƒë·ªïi ({label_changed/len(df_with_pattern)*100:.1f}%)")
    
#     # Save data
#     output_file = 'D:\\py\\ArrowPuzzle\\difficulty_plots\\data_with_pattern.csv'
#     df_with_pattern.to_csv(output_file, index=False)
#     print(f"\n‚úì ƒê√£ l∆∞u d·ªØ li·ªáu: data_with_pattern.csv")
    
#     # T·∫°o summary metrics
#     summary = {
#         'Metric': ['Difficulty', 'Difficulty Label'],
#         'Original_MAE': [original_diff_wr_mae, original_label_wr_mae],
#         'Pattern_MAE': [metrics_diff_wr['mae'], metrics_label_wr['mae']],
#         'MAE_Change': [change_diff, change_label],
#         'MAE_Change_%': [pct_diff, pct_label],
#         'Original_Pearson_r': [original_diff_wr_corr, original_label_wr_corr],
#         'Pattern_Pearson_r': [metrics_diff_wr['pearson_r'], metrics_label_wr['pearson_r']],
#         'Levels_Changed': [diff_changed, label_changed]
#     }
    
#     summary_df = pd.DataFrame(summary)
#     summary_file = 'D:\\py\\ArrowPuzzle\\difficulty_plots\\pattern_summary.csv'
#     summary_df.to_csv(summary_file, index=False)
#     print(f"‚úì ƒê√£ l∆∞u summary: pattern_summary.csv")
    
#     return df_with_pattern, summary_df

# # ================================
# # MAIN EXECUTION
# # ================================
# if __name__ == "__main__":
#     # Load data
#     filepath = "D:\\py\\ArrowPuzzle\\[ArrowPuzzle] ƒê√°nh gi√° ƒë·ªô kh√≥ - Sheet2.csv"
    
#     print("="*80)
#     print("B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH PATTERN")
#     print("="*80)
    
#     df = load_and_prepare_data(filepath, num_levels=50)
#     print(f"\n‚úì ƒê√£ load {len(df)} levels")
    
#     # Ch·∫°y ph√¢n t√≠ch pattern
#     df_with_pattern, summary = analyze_pattern_comprehensive(df)
    
#     # Hi·ªÉn th·ªã summary cu·ªëi c√πng
#     print("\n" + "="*80)
#     print("SUMMARY TABLE")
#     print("="*80)
#     print(summary.to_string(index=False))
    
#     print("\n" + "="*80)
#     print("‚úÖ HO√ÄN T·∫§T!")
#     print("="*80)
#     print("\nƒê√£ t·∫°o c√°c file:")
#     print("  1. comparison_difficulty_original_vs_pattern.png")
#     print("  2. comparison_DifficultyLabel_Num_original_vs_pattern.png")
#     print("  3. FINAL_pattern_difficulty_vs_winrate.png")
#     print("  4. FINAL_pattern_DifficultyLabel_Num_vs_winrate.png")
#     print("  5. data_with_pattern.csv")
#     print("  6. pattern_summary.csv")
#     print("="*80)
